{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b141cae-aab7-412f-a673-69fa1deabfc3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d6e36e27-5289-422e-8d2d-69d375d650cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, BartTokenizer, BartForConditionalGeneration,  BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import textwrap\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bfa1c440-ed15-4ed7-94f2-eaaa5d31865d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ae09f3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "\n",
    "with open('articles.json', 'r', encoding='utf-8') as f:\n",
    "    articles = json.load(f)  \n",
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "def summarize_text(text, max_len=80):\n",
    "    inputs = tokenizer([text], max_length=1024, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=max_len, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9b106-1d22-4e13-9c09-d8980c72acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [summarize_text(article['text']) for article in articles]\n",
    "\n",
    "with open(\"summaries.txt\", \"w\", encoding='utf-8') as file:\n",
    "    for summary in summaries:\n",
    "        file.write(f\"{summary}\\n\")\n",
    "\n",
    "print(\"summaries.txt generated\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60c8b92-9174-421c-a105-8ad71fd9563b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragon Kitty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dragon Kitty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The importance of climate change is increasing globally. Governments and organizations are focusing more on reducing carbon emissions. Renewable energy sources are becoming more cost-effective and widely used.\n",
      "\n",
      "The United Nations Framework Convention on Climate Change (UNFCCC) is an international agreement that aims to limit global warming to 2 degrees Celsius (3.6 degrees Fahrenheit) above pre-industrial levels by the end of this century. The agreement was signed in Paris in December 2015, and has been ratified by more than 190 countries. It sets a goal of limiting global temperature rise to 1.5 degrees C (2.7 degrees F) by 2100, with a target of keeping global average temperatures from rising above that level for at least the next 150 years. This goal is based on the assumption that emissions of carbon dioxide, methane, nitrous oxide, hydrofluorocarbons (HFCs), and other greenhouse gases will continue to decline over the course of the 21st century, as they have in the past.\n"
     ]
    }
   ],
   "source": [
    "def generate_article(summary_sentences, min_words=200, max_words=1000, model_name='gpt2-medium'):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Encode the summary sentences\n",
    "    input_text = \" \".join(summary_sentences)\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_words,\n",
    "        min_length=min_words,\n",
    "        num_beams=5,   # Controls the quality of the generated text\n",
    "        no_repeat_ngram_size=2,  # Avoids repetition\n",
    "        early_stopping=True,\n",
    "        temperature=0.7,  # Controls randomness, lower is more focused\n",
    "        top_k=50,  # Limits the sampling pool\n",
    "        top_p=0.95  # Nucleus sampling\n",
    "    )\n",
    "\n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return the generated article\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea5b5be9-542d-4830-9c94-fe1f77844299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Donald Trump lost. But Trumpism did not. It won in the parts of the country and with the voters whom Trump catered to. Joe Biden defeated Trump to win the presidency, and is on pace to win up to 306 electoral votes. In a typical election year, such a victory would mean Biden would have carried other Democrats along with him.\n",
      "\n",
      "This is not to say that Trump's victory was a foregone conclusion. There are plenty of reasons to be skeptical of his victory, including the fact that he won the popular vote but lost the Electoral College. And there is no reason to believe that Democrats will be able to recapture the House of Representatives in 2018, which would be the first time since the Civil War that the party has won back the White House since Reconstruction. Still, it is important to remember that this election was not a referendum on Trump, or even on the Republican Party, but rather a contest between two very different visions for the future of American politics. The Democratic Party has a long way to go before it can claim to represent the interests of working people and the middle class.\n"
     ]
    }
   ],
   "source": [
    "summary_sentences = [\"President Donald Trump lost.\", \"But Trumpism did not.\",\"It won in the parts of the country and with the voters whom Trump catered to. Joe Biden defeated Trump to win the presidency, and is on pace to win up to 306 electoral votes.\",\n",
    "                     \"In a typical election year, such a victory would mean Biden would have carried other Democrats along with him.\"\n",
    "]\n",
    "\n",
    "article1 = generate_article(summary_sentences)\n",
    "print(article1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e86138fd-be10-4e02-b23f-6c98d4b7fa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragon Kitty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dragon Kitty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumana Azam was working 16-hour days responding to an influx of coronavirus patients. During the worst of it, the 34-year-old respiratory therapist was facing multiple deaths a day. Her hours have decreased since then, but another major event in Azam's life ended this year.\n",
      "\n",
      "Azam, who was born in Pakistan, was diagnosed with acute respiratory syndrome, or ARDS, when she was 4 years old. The condition is characterized by fever, cough, and shortness of breath, which can last for days or even weeks. It's caused by a virus that infects the airways, causing inflammation and inflammation of the lungs, leading to pneumonia, bronchitis, emphysema, sinusitis and even death. In the United States, more than 1,000 people die each year from the virus, according to the Centers for Disease Control and Prevention (CDC). The virus is spread through close contact with an infected person, such as coughing, sneezing or touching a sick person's face or mouth. A person can also contract the disease from another person who is infected with the same virus. People who are infected can spread the infection to others through direct contact, through sharing needles or other syringes, by sharing food or drink contaminated with infected blood or by touching infected surfaces or objects.\n"
     ]
    }
   ],
   "source": [
    "summary_sentences = [\"Jumana Azam was working 16-hour days responding to an influx of coronavirus patients.\", \n",
    "                     \"During the worst of it, the 34-year-old respiratory therapist was facing multiple deaths a day.\",\n",
    "\"Her hours have decreased since then, but another major event in Azam's life ended this year.\"]\n",
    "\n",
    "article2 = generate_article(summary_sentences)\n",
    "print(article2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e8a0eff1-8dec-4474-b770-9dd1ec64f419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dragon Kitty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Dragon Kitty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the children died soon after being rescued, while a fourth child was still trapped. The Friday afternoon quake that struck Turkey’s Aegean coast and north of the Greek island of Samos registered a magnitude of 6.6. It toppled buildings in Izmir and triggered a small tsunami in the Seferihisar district.\n",
      "\n",
      "Turkey's Prime Minister Ahmet Davutoğlu said the death toll was expected to rise as rescuers continued to search for survivors. \"We are trying to find as many people as we can,\" he said, according to the state-run Anadolu news agency. He added that rescue teams had found the body of a woman who had been trapped in a collapsed building, but did not say whether she was among the dead. A rescue official told Reuters that the woman was believed to be in her late 20s or early 30s and was wearing a headscarf when she fell into the rubble. She was taken to a hospital for treatment, the official said.\n"
     ]
    }
   ],
   "source": [
    "summary_sentences = [\"One of the children died soon after being rescued, while a fourth child was still trapped.\",\n",
    "                    \"The Friday afternoon quake that struck Turkey’s Aegean coast and north of the Greek island of Samos registered a magnitude of 6.6.\",\n",
    "                    \"It toppled buildings in Izmir and triggered a small tsunami in the Seferihisar district.\"]\n",
    "\n",
    "article3 = generate_article(summary_sentences)\n",
    "print(article3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "76b2475a-fce6-42bd-9ee9-3bbc375c65ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the children died soon after being rescued, while a fourth child was still trapped. The Friday afternoon quake that struck Turkey’s Aegean coast and north of the Greek island of Samos registered a magnitude of 6.6. It toppled buildings in Izmir and triggered a small tsunami in the Seferihisar district.\n",
      "\n",
      "Turkey's Prime Minister Ahmet Davutoğlu said the death toll was expected to rise as rescuers continued to search for survivors. \"We are trying to find as many people as we can,\" he said, according to the state-run Anadolu news agency. He added that rescue teams had found the body of a woman who had been trapped in a collapsed building, but did not say whether she was among the dead. A rescue official told Reuters that the woman was believed to be in her late 20s or early 30s and was wearing a headscarf when she fell into the rubble. She was taken to a hospital for treatment, the official said.\n"
     ]
    }
   ],
   "source": [
    "summary_sentences = [\"Nations are scrambling to ramp up vaccination campaigns in hopes of stemming the tide of infections.\",\n",
    "                     \"Pfizer-BioNTech and Moderna vaccines, both based on new mRNA technology, have been approved for emergency use in multiple countries\",\n",
    "\"Other vaccines, such as the Oxford-AstraZeneca vaccine, are in the final stages of approval.\",\n",
    "\"distribution networks have been tested by the need to store and transport vaccines at extremely low temperatures.\",\n",
    " \"In many low-income countries, there are concerns about access to cold-chain infrastructure.\",\n",
    "                     \"In some countries, misinformation and distrust of governments and pharmaceutical companies have led to doubts about the safety of the vaccines.\"]\n",
    "\n",
    "article4 = generate_article(summary_sentences)\n",
    "print(article4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "72735479-17ac-40a5-aaef-6b71154a1ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California is enduring its worst wildfire season on record. More than 3.1 million acres have been scorched and dozens of major blazes are still active. The devastation has already claimed at least 25 lives, destroyed thousands of homes, and displaced hundreds of thousands of residents. \"This is a crisis unlike anything we’ve seen,\" said Cal Fire Chief Thom Porter.\n",
      "\n",
      "The California Department of Forestry and Fire Protection (Cal Fire) has declared a state of emergency for the entire state, as well as the San Joaquin Valley, Kern County, Sacramento, San Francisco Bay Area, Northern California, Southern California and Northern Nevada. In addition, the National Weather Service has issued a severe thunderstorm watch for much of the state and the Pacific Northwest, including Washington, Oregon, Idaho, Montana, Nevada, Colorado, Utah, Arizona, New Mexico, Texas, Oklahoma, South Dakota, Nebraska, Kansas, Illinois, Indiana, Michigan, Wisconsin, Minnesota, Iowa, Missouri, Kentucky, Tennessee, Georgia, North Carolina, Alabama, Mississippi, Arkansas, Louisiana, Florida, West Virginia, Ohio, Pennsylvania, Delaware, Maryland, Massachusetts, Rhode Island, Vermont, Virginia and New Hampshire.\n"
     ]
    }
   ],
   "source": [
    "summary_sentences = [\"California is enduring its worst wildfire season on record.\",\n",
    "                     \"More than 3.1 million acres have been scorched and dozens of major blazes are still active.\",\n",
    "\"The devastation has already claimed at least 25 lives, destroyed thousands of homes, and displaced hundreds of thousands of residents.\",\n",
    "                     \"\\\"This is a crisis unlike anything we’ve seen,\\\" said Cal Fire Chief Thom Porter.\"]\n",
    "\n",
    "article5 = generate_article(summary_sentences)\n",
    "print(article5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "9dc47c43-72d0-4491-83ee-52d0b1e737e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margaret Keenan, 90, became the first person in the world to receive the Pfizer vaccine outside of clinical trials. Officials have emphasized that patience will be needed before vaccines can bring an end to the pandemic. \"This is a great Christmas present,\" Keenan says.\n",
      "\n",
      "Keenan and her husband, John, have lived in New York City for more than 40 years. They have two children, a boy and a girl, and they have never had a flu shot. But when they heard about the vaccine, they knew they had to get it for their son, who has a rare form of the flu, called H1N1, which can cause severe illness and death. The vaccine is given to children between the ages of 6 months and 12 years old, but it can also be given at any age to adults who are at high risk of contracting the disease, such as pregnant women, people with weakened immune systems, or people who have recently traveled to an area where the virus has been circulating for a prolonged period of time.\n"
     ]
    }
   ],
   "source": [
    "summary_sentences = [\"Margaret Keenan, 90, became the first person in the world to receive the Pfizer vaccine outside of clinical trials.\",\n",
    "                     \"Officials have emphasized that patience will be needed before vaccines can bring an end to the pandemic.\",\n",
    "                     \"\\\"This is a great Christmas present,\\\" Keenan says.\"]\n",
    "\n",
    "article6 = generate_article(summary_sentences)\n",
    "print(article6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "20127042-cfb2-4c1f-8a00-0becc08575b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "def compute_perplexity(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        loss = outputs.loss\n",
    "    return torch.exp(loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "935a38dd-3cd2-4a9e-b43d-adaa33839654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_burstiness(text, tokenizer):\n",
    "    tokenized = tokenizer.encode(text)\n",
    "    token_counts = Counter(tokenized)\n",
    "    frequencies = list(token_counts.values())\n",
    "    if len(frequencies) > 1:\n",
    "        mean_freq = np.mean(frequencies)\n",
    "        variance = np.var(frequencies)\n",
    "        burstiness_score = variance / mean_freq if mean_freq != 0 else 0\n",
    "    else:\n",
    "        burstiness_score = 0 \n",
    "    \n",
    "    return burstiness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e74109-055a-4337-9ba0-76ff19a96e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 12.000144004821777\n",
      "Burstiness: 2.3336466165413534\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(article1, gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(article1, gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")\n",
    "perplexity = compute_perplexity(articles[0]['text'], gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(articles[0]['text'], gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3e4342b6-4a7c-4ae4-ac33-e04ea3f2f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 11.710691452026367\n",
      "Burstiness: 2.4543227856480865\n",
      "Perplexity: 43.00416564941406\n",
      "Burstiness: 1.288589599700711\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(article2, gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(article2, gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")\n",
    "perplexity = compute_perplexity(articles[10]['text'], gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(articles[10]['text'], gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2f644236-f74b-4566-8ece-4c046f891aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 11.565011024475098\n",
      "Burstiness: 1.2923367083581494\n",
      "Perplexity: 21.6721248626709\n",
      "Burstiness: 1.0806883675623025\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(article3, gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(article3, gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")\n",
    "perplexity = compute_perplexity(articles[20]['text'], gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(articles[20]['text'], gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f9069944-87be-4e4b-b84d-4109efc9c883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 35.59250259399414\n",
      "Burstiness: 2.3445199660152927\n",
      "Perplexity: 21.9725341796875\n",
      "Burstiness: 1.614270685067145\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(article4, gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(article4, gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")\n",
    "perplexity = compute_perplexity(articles[96]['text'], gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(articles[96]['text'], gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bea88e7c-c2df-4640-8d2a-4e99a0758481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 10.235406875610352\n",
      "Burstiness: 9.665434173669471\n",
      "Perplexity: 13.818424224853516\n",
      "Burstiness: 1.268199233716475\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(article5, gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(article5, gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")\n",
    "perplexity = compute_perplexity(articles[92]['text'], gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(articles[92]['text'], gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7e2982a4-9b28-4adb-9955-0b609281557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 10.209077835083008\n",
      "Burstiness: 1.7292312024781757\n",
      "Perplexity: 19.286537170410156\n",
      "Burstiness: 0.348358585858586\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(article6, gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(article6, gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")\n",
    "perplexity = compute_perplexity(articles[99]['text'], gpt2_model, gpt2_tokenizer, device)\n",
    "burstiness = compute_burstiness(articles[99]['text'], gpt2_tokenizer)\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "print(f\"Burstiness: {burstiness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f2b19f0b-5e83-42e6-9676-4c1a34658313",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [article[\"text\"] for article in articles]\n",
    "labels = [0] * 50 + [1] * 50  # 1 = AI, 0 = Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e586b88c-c178-49e2-84cd-7878682aa028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6766 | Val Accuracy = 0.6000\n",
      "Epoch 2: Train Loss = 0.6348 | Val Accuracy = 0.8500\n",
      "Epoch 3: Train Loss = 0.5367 | Val Accuracy = 0.9000\n",
      "Epoch 4: Train Loss = 0.3993 | Val Accuracy = 0.8500\n",
      "Epoch 5: Train Loss = 0.3649 | Val Accuracy = 0.9500\n",
      "Epoch 6: Train Loss = 0.2641 | Val Accuracy = 0.9500\n",
      "Epoch 7: Train Loss = 0.2095 | Val Accuracy = 1.0000\n",
      "Epoch 8: Train Loss = 0.1671 | Val Accuracy = 0.9500\n",
      "Epoch 9: Train Loss = 0.1097 | Val Accuracy = 1.0000\n",
      "Epoch 10: Train Loss = 0.0809 | Val Accuracy = 1.0000\n",
      "Epoch 11: Train Loss = 0.0700 | Val Accuracy = 1.0000\n",
      "Epoch 12: Train Loss = 0.0444 | Val Accuracy = 0.9500\n",
      "Epoch 13: Train Loss = 0.0339 | Val Accuracy = 1.0000\n",
      "Epoch 14: Train Loss = 0.0296 | Val Accuracy = 1.0000\n",
      "Epoch 15: Train Loss = 0.0237 | Val Accuracy = 1.0000\n",
      "Epoch 16: Train Loss = 0.0193 | Val Accuracy = 1.0000\n",
      "Epoch 17: Train Loss = 0.0154 | Val Accuracy = 1.0000\n",
      "Epoch 18: Train Loss = 0.0136 | Val Accuracy = 1.0000\n",
      "Epoch 19: Train Loss = 0.0128 | Val Accuracy = 1.0000\n",
      "Epoch 20: Train Loss = 0.0111 | Val Accuracy = 1.0000\n",
      "Epoch 21: Train Loss = 0.0104 | Val Accuracy = 1.0000\n",
      "Epoch 22: Train Loss = 0.0088 | Val Accuracy = 1.0000\n",
      "Epoch 23: Train Loss = 0.0084 | Val Accuracy = 1.0000\n",
      "Epoch 24: Train Loss = 0.0078 | Val Accuracy = 1.0000\n",
      "Epoch 25: Train Loss = 0.0067 | Val Accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts[:], labels[:] = zip(*combined)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.dropout(out.pooler_output)\n",
    "        return self.fc(pooled)\n",
    "\n",
    "def train_model(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "train_ds = TextDataset(train_texts, train_labels, tokenizer)\n",
    "val_ds = TextDataset(val_texts, val_labels, tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(25):\n",
    "    train_loss = train_model(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_acc = evaluate_model(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f} | Val Accuracy = {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "baf43d7d-c4ee-4aa7-a0d7-f675c5c6aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, texts, device, max_len=128):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            enc = tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = enc['input_ids'].to(device)\n",
    "            attention_mask = enc['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            pred = torch.argmax(outputs, dim=1).item()\n",
    "            predictions.append(pred)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "90754bb6-d0ce-4b1e-9fec-3ed3b6465129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Human] Human\n",
      "[Human] Human\n",
      "[Human] Human\n",
      "[Human] Human\n",
      "[AI] Human\n",
      "[AI] AI\n",
      "[AI] AI\n",
      "[AI] AI\n",
      "[AI] AI\n",
      "[AI] AI\n"
     ]
    }
   ],
   "source": [
    "with open('testarticles.json', 'r', encoding='utf-8') as f:\n",
    "    testarticles = json.load(f)  \n",
    "\n",
    "test_texts = [test[\"text\"] for test in testarticles]\n",
    "truevalue = [\"Human\",\"Human\",\"Human\",\"Human\",\"Human\",\"AI\",\"AI\",\"AI\",\"AI\",\"AI\"]\n",
    "# Predict (0 = human, 1 = AI)\n",
    "i = 0\n",
    "predicted_classes = predict(model, tokenizer, test_texts, device)\n",
    "for text, pred in zip(test_texts, predicted_classes):\n",
    "    label = \"AI\" if pred == 1 else \"Human\"\n",
    "    print(f\"[{label}] {truevalue[i]}\")\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0dff6778-0293-4973-9b9f-3183311dc15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.33%\n",
      "[Human] Human (49.05767059326172)\n",
      "[AI] Human (27.24824333190918)\n",
      "[Human] Human (61.7662353515625)\n",
      "[Human] Human (50.51237487792969)\n",
      "[AI] Human (13.234869956970215)\n",
      "[AI] AI (27.44577407836914)\n",
      "[AI] AI (23.773452758789062)\n",
      "[AI] AI (29.323810577392578)\n",
      "[AI] AI (32.390220642089844)\n",
      "[AI] AI (18.873767852783203)\n"
     ]
    }
   ],
   "source": [
    "perplexities = [compute_perplexity(text, gpt2_model, gpt2_tokenizer, device) for text in texts]\n",
    "\n",
    "X = np.array(perplexities).reshape(-1, 1)  \n",
    "y = np.array(labels)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "test_texts = [test[\"text\"] for test in testarticles]\n",
    "truevalue = [\"Human\",\"Human\",\"Human\",\"Human\",\"Human\",\"AI\",\"AI\",\"AI\",\"AI\",\"AI\"]\n",
    "\n",
    "i=0\n",
    "for text in test_texts:\n",
    "    new_perplexity = compute_perplexity(text, gpt2_model, gpt2_tokenizer, device)\n",
    "    pred = classifier.predict([[new_perplexity]])[0]\n",
    "    label = \"AI\" if pred == 1 else \"Human\"\n",
    "    print(f\"[{label}] {truevalue[i]} ({new_perplexity})\")\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "816d9db1-01ff-41f4-9241-0dd12dd11118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.9396 | Val Accuracy = 0.4000\n",
      "Epoch 2: Train Loss = 0.8129 | Val Accuracy = 0.5500\n",
      "Epoch 3: Train Loss = 0.6868 | Val Accuracy = 0.5000\n",
      "Epoch 4: Train Loss = 0.5566 | Val Accuracy = 0.6500\n",
      "Epoch 5: Train Loss = 0.4195 | Val Accuracy = 0.7500\n",
      "Epoch 6: Train Loss = 0.3240 | Val Accuracy = 0.6000\n",
      "Epoch 7: Train Loss = 0.2519 | Val Accuracy = 0.8000\n",
      "Epoch 8: Train Loss = 0.1931 | Val Accuracy = 0.9000\n",
      "Epoch 9: Train Loss = 0.1482 | Val Accuracy = 0.9000\n",
      "Epoch 10: Train Loss = 0.1125 | Val Accuracy = 0.8000\n",
      "Epoch 11: Train Loss = 0.0967 | Val Accuracy = 0.8000\n",
      "Epoch 12: Train Loss = 0.0824 | Val Accuracy = 0.8000\n",
      "Epoch 13: Train Loss = 0.0714 | Val Accuracy = 0.9000\n",
      "Epoch 14: Train Loss = 0.0636 | Val Accuracy = 0.9000\n",
      "Epoch 15: Train Loss = 0.0649 | Val Accuracy = 0.8500\n",
      "Epoch 16: Train Loss = 0.0525 | Val Accuracy = 0.8500\n",
      "Epoch 17: Train Loss = 0.0670 | Val Accuracy = 0.8000\n",
      "Epoch 18: Train Loss = 0.0444 | Val Accuracy = 0.8000\n",
      "Epoch 19: Train Loss = 0.0511 | Val Accuracy = 0.8000\n",
      "Epoch 20: Train Loss = 0.0450 | Val Accuracy = 0.8000\n",
      "Epoch 21: Train Loss = 0.0411 | Val Accuracy = 0.8000\n",
      "Epoch 22: Train Loss = 0.0396 | Val Accuracy = 0.8000\n",
      "Epoch 23: Train Loss = 0.0451 | Val Accuracy = 0.8000\n",
      "Epoch 24: Train Loss = 0.0447 | Val Accuracy = 0.8000\n",
      "Epoch 25: Train Loss = 0.0466 | Val Accuracy = 0.8000\n"
     ]
    }
   ],
   "source": [
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts[:], labels[:] = zip(*combined)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, gpt2_model, gpt2_tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.gpt2_model = gpt2_model\n",
    "        self.gpt2_tokenizer = gpt2_tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        perplexity = compute_perplexity(text, self.gpt2_model, self.gpt2_tokenizer, device)\n",
    "\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            'perplexity': torch.tensor(perplexity, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size + 1, 2)  # Add 1 to dimension for perplexity feature\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, perplexity):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.dropout(out.pooler_output)\n",
    "        \n",
    "        x = torch.cat((pooled, perplexity.unsqueeze(1)), dim=1)  # Add 1 to dimension for perplexity feature\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_model(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        perplexity = batch['perplexity'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, perplexity)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            perplexity = batch['perplexity'].to(device)\n",
    "            outputs = model(input_ids, attention_mask, perplexity)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "train_ds = TextDataset(train_texts, train_labels, bert_tokenizer, gpt2_model, gpt2_tokenizer)\n",
    "val_ds = TextDataset(val_texts, val_labels, bert_tokenizer, gpt2_model, gpt2_tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(25):\n",
    "    train_loss = train_model(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_acc = evaluate_model(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f} | Val Accuracy = {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61a434ce-b56c-4efb-a740-caca7250f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, gpt2_model, gpt2_tokenizer, device, max_len=128):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    perplexity = compute_perplexity(text, gpt2_model, gpt2_tokenizer, device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, torch.tensor([perplexity], dtype=torch.float).to(device))\n",
    "        \n",
    "        predicted_label = torch.argmax(outputs, dim=1).item()\n",
    "    \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f97a902d-1529-4ae5-890a-c7c9c9988145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Human] Human\n",
      "[Human] Human\n",
      "[Human] Human\n",
      "[AI] Human\n",
      "[AI] Human\n",
      "[AI] AI\n",
      "[Human] AI\n",
      "[AI] AI\n",
      "[AI] AI\n",
      "[AI] AI\n"
     ]
    }
   ],
   "source": [
    "with open('testarticles.json', 'r', encoding='utf-8') as f:\n",
    "    testarticles = json.load(f)\n",
    "\n",
    "test_texts = [test[\"text\"] for test in testarticles]\n",
    "\n",
    "truevalue = [\"Human\", \"Human\", \"Human\", \"Human\", \"Human\", \"AI\", \"AI\", \"AI\", \"AI\", \"AI\"]\n",
    "\n",
    "predicted_classes = []\n",
    "for text in test_texts:\n",
    "    pred = predict(text, model, bert_tokenizer, gpt2_model, gpt2_tokenizer, device)\n",
    "    predicted_classes.append(pred)\n",
    "i = 0\n",
    "for text, pred in zip(test_texts, predicted_classes):\n",
    "    label = \"AI\" if pred == 1 else \"Human\"\n",
    "    print(f\"[{label}] {truevalue[i]}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7eca1e41-0c08-4f81-85e9-b040f571dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Perplexity: 37.7479\n",
      "AI Perplexity: 29.0534\n"
     ]
    }
   ],
   "source": [
    "htext, aitext = texts[:50], texts[50:]\n",
    "for text in htext:\n",
    "    perplexity += compute_perplexity(text, gpt2_model, gpt2_tokenizer, device)\n",
    "perplexity /= 50\n",
    "print(f\"Human Perplexity: {perplexity:.4f}\")\n",
    "for  text in aitext:\n",
    "    perplexity += compute_perplexity(text, gpt2_model, gpt2_tokenizer, device)\n",
    "perplexity /= 50\n",
    "print(f\"AI Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9becd483-ae1c-4dd8-ad66-d68d858fb4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burstiness: 2.8731\n",
      "Burstiness: 2.0008\n"
     ]
    }
   ],
   "source": [
    "for text in htext:\n",
    "    burstiness_score += compute_burstiness(text, tokenizer)\n",
    "burstiness_score /= 50\n",
    "print(f\"Burstiness: {burstiness_score:.4f}\")\n",
    "for text in aitext:\n",
    "    burstiness_score += compute_burstiness(text, tokenizer)\n",
    "burstiness_score /= 50\n",
    "print(f\"Burstiness: {burstiness_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0c9d4aab-d05c-48ed-ac46-8328569e2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, gpt2_model, gpt2_tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.gpt2_model = gpt2_model\n",
    "        self.gpt2_tokenizer = gpt2_tokenizer\n",
    "        self.perplexities = [compute_perplexity(t,gpt2_model, gpt2_tokenizer, device) for t in texts]\n",
    "        self.burstinesses = [compute_burstiness(t, gpt2_tokenizer) for t in texts]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        enc = self.tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=self.max_len,\n",
    "        return_tensors='pt'\n",
    "        )\n",
    "    \n",
    "        perplexity = compute_perplexity(text, self.gpt2_model, self.gpt2_tokenizer, device)\n",
    "        burstiness = compute_burstiness(text, self.tokenizer)\n",
    "\n",
    "        return {\n",
    "        'input_ids': enc['input_ids'].squeeze(0),\n",
    "        'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "        'label': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        'perplexity': torch.tensor(perplexity, dtype=torch.float),\n",
    "        'burstiness': torch.tensor(burstiness, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2a2a7ad3-867c-4dcc-8fc4-01c7d230c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size + 2, 2) # Add 2 for perplexity and burstiness\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, perplexity, burstiness):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.dropout(out.pooler_output)\n",
    "\n",
    "        x = torch.cat((pooled, perplexity.unsqueeze(1), burstiness.unsqueeze(1)), dim=1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f97243ac-e4a8-4ff3-9570-731e5afe8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        perplexity = batch['perplexity'].to(device)\n",
    "        burstiness = batch['burstiness'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, perplexity, burstiness)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "69159044-c8c2-4d8d-85c5-bca3f754408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            perplexity = batch['perplexity'].to(device)\n",
    "            burstiness = batch['burstiness'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, perplexity, burstiness)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "87578434-c682-483e-b9d6-1a8fe4b72a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.6979 | Val Accuracy = 0.7500\n",
      "Epoch 2: Train Loss = 0.6024 | Val Accuracy = 0.7000\n",
      "Epoch 3: Train Loss = 0.4814 | Val Accuracy = 0.8000\n",
      "Epoch 4: Train Loss = 0.2971 | Val Accuracy = 0.9000\n",
      "Epoch 5: Train Loss = 0.2012 | Val Accuracy = 0.9500\n",
      "Epoch 6: Train Loss = 0.1399 | Val Accuracy = 0.9500\n",
      "Epoch 7: Train Loss = 0.0878 | Val Accuracy = 0.9500\n",
      "Epoch 8: Train Loss = 0.0737 | Val Accuracy = 0.9500\n",
      "Epoch 9: Train Loss = 0.0554 | Val Accuracy = 0.9500\n",
      "Epoch 10: Train Loss = 0.0429 | Val Accuracy = 0.9500\n",
      "Epoch 11: Train Loss = 0.0393 | Val Accuracy = 0.9500\n",
      "Epoch 12: Train Loss = 0.0349 | Val Accuracy = 0.9500\n",
      "Epoch 13: Train Loss = 0.0281 | Val Accuracy = 0.9500\n",
      "Epoch 14: Train Loss = 0.0263 | Val Accuracy = 0.9500\n",
      "Epoch 15: Train Loss = 0.0223 | Val Accuracy = 0.9500\n",
      "Epoch 16: Train Loss = 0.0203 | Val Accuracy = 0.9500\n",
      "Epoch 17: Train Loss = 0.0206 | Val Accuracy = 0.9500\n",
      "Epoch 18: Train Loss = 0.0194 | Val Accuracy = 0.9500\n",
      "Epoch 19: Train Loss = 0.0186 | Val Accuracy = 0.9500\n",
      "Epoch 20: Train Loss = 0.0161 | Val Accuracy = 0.9500\n",
      "Epoch 21: Train Loss = 0.0136 | Val Accuracy = 0.9500\n",
      "Epoch 22: Train Loss = 0.0125 | Val Accuracy = 0.9500\n",
      "Epoch 23: Train Loss = 0.0131 | Val Accuracy = 0.9500\n",
      "Epoch 24: Train Loss = 0.0109 | Val Accuracy = 0.9500\n",
      "Epoch 25: Train Loss = 0.0101 | Val Accuracy = 0.9500\n"
     ]
    }
   ],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "train_ds = TextDataset(train_texts, train_labels, bert_tokenizer, gpt2_model, gpt2_tokenizer, max_len=128)\n",
    "val_ds = TextDataset(val_texts, val_labels, bert_tokenizer, gpt2_model, gpt2_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=16)\n",
    "\n",
    "model = BERTClassifier().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(25):\n",
    "    train_loss = train_model(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_acc = evaluate_model(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f} | Val Accuracy = {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1aaa479a-6526-4ae3-8391-7730b6230ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, tokenizer, gpt2_model, gpt2_tokenizer, device, max_len=128):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    perplexity = compute_perplexity(text, gpt2_model, gpt2_tokenizer, device)\n",
    "\n",
    "    burstiness = compute_burstiness(text, tokenizer)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "\n",
    "        outputs = model(input_ids, attention_mask, \n",
    "                         torch.tensor([perplexity], dtype=torch.float).to(device),\n",
    "                         torch.tensor([burstiness], dtype=torch.float).to(device))\n",
    "        \n",
    "\n",
    "        predicted_label = torch.argmax(outputs, dim=1).item()\n",
    "    \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "60fb6721-e537-4f98-b5d0-c4e45ce98e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Human] Human\n",
      "[Human] Human\n",
      "[Human] Human\n",
      "[Human] Human\n",
      "[AI] Human\n",
      "[Human] AI\n",
      "[Human] AI\n",
      "[AI] AI\n",
      "[AI] AI\n",
      "[AI] AI\n"
     ]
    }
   ],
   "source": [
    "with open('testarticles.json', 'r', encoding='utf-8') as f:\n",
    "    testarticles = json.load(f)\n",
    "\n",
    "test_texts = [test[\"text\"] for test in testarticles]\n",
    "\n",
    "truevalue = [\"Human\", \"Human\", \"Human\", \"Human\", \"Human\", \"AI\", \"AI\", \"AI\", \"AI\", \"AI\"]\n",
    "\n",
    "predicted_classes = []\n",
    "for text in test_texts:\n",
    "    pred = predict(text, model, bert_tokenizer, gpt2_model, gpt2_tokenizer, device)\n",
    "    predicted_classes.append(pred)\n",
    "i = 0\n",
    "for text, pred in zip(test_texts, predicted_classes):\n",
    "    label = \"AI\" if pred == 1 else \"Human\"\n",
    "    print(f\"[{label}] {truevalue[i]}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacfc1c9-bc36-4c68-9343-a2480c709401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f5e84-11c8-463b-8138-053f4e78ac66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
